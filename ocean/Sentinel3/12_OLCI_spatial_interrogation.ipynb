{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/LogoWekeo_Copernicus_RGB_0.png' alt='' align='centre' width='30%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLCI spatial plotting, quality control and data interrogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Version: 3.0\n",
    "    Date:    10/07/2020\n",
    "    Author:  Ben Loveday (InnoFlair / Plymouth Marine Laboratory) and Hayley Evers-King (EUMETSAT)\n",
    "    Credit:  This code was developed for EUMETSAT under contracts for the European Commission Copernicus \n",
    "             programme.\n",
    "    License: This code is offered as open source and free-to-use in the public domain, \n",
    "             with no warranty, under the MIT license associated with this code repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This aim of this code is to introduce you to Python and to simply import a netCDF file in to your python workspace, conduct some basic operations, and plot an image. In this case, we will be using a level-2 OLCI image, but the script can be easily adapted to plot any netCDF variable.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Get the WEkEO User credentials</b>\n",
    "<hr>\n",
    "If you want to download the data to use this notebook, you will need WEkEO User credentials. If you do not have these, you can register <a href=\"https://www.wekeo.eu/web/guest/user-registration\" target=\"_blank\">here</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in any python code is usually to import libraries that you will need. Libraries are usually code modules that perform specific tasks or provide specific capability (e.g. statistical analysis or plotting routines). In this case we will import the xarray library for handling netCDF files, the numpy library which will help to conduct various operations on the data, and the matplotlib plotting library to generate some images. We will also import the os library, that allows python access to some command-line-eqsue capability like 'list directory', as well as the python library that governs the reporting of warning (so that we can turn them off here, and make the code run without being so 'noisy')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# libraries are imported here, and we can import any library with an alias that allows us easy access to them later.\n",
    "import sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from IPython.core.display import display, HTML\n",
    "import json\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to import the WEkEO Harmonised Data Access API, which will allow us to get remote data if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())),'wekeo-hda'))\n",
    "import hda_api_functions as hapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we also define functions at the top of a Python script. Functions are routines that can be called elsewhere in our script and perform a specific task. Typically we would use a function to take care of any process that we are going to perform more than once. The box below defines a function that will mask our data according to quality flags. We will call this function later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_data_fast(flags_we_want, flag_names, flag_values, flag_data, flag_type='WQSF'):\n",
    "    flag_bits = np.uint64()\n",
    "    if flag_type == 'SST':\n",
    "        flag_bits = np.uint8()\n",
    "    elif flag_type == 'WQSF_lsb':\n",
    "        flag_bits = np.uint32()\n",
    "    \n",
    "    for flag in flags_we_want:\n",
    "        try:\n",
    "            flag_bits = flag_bits | flag_values[flag_names.index(flag)]\n",
    "        except:\n",
    "            print(flag + \" not present\")\n",
    "    \n",
    "    return (flag_data & flag_bits) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will start our script, proper.\n",
    "\n",
    "To run this script, we will download some data from WEkEO harmonised data access. WEkEO provides access to a huge number of datasets through its **'harmonised-data-access'** API. This allows us to query the full data catalogue and download data quickly and directly onto the Jupyter Lab. You can search for what data is available <a href=\"https://wekeo.eu/data?view=catalogue\">here</a>\n",
    "\n",
    "In order to use the HDA-API we need to provide some authentication credentials, which comes in the form of an API key and API token. In this notebook we have provided functions so you can retrieve the API key and token you need directly. You can find out more about this process in the notebook on HDA access (wekeo_harmonized_data_access_api.ipynb) that can be found in the **wekeo-hda** folder on your Jupyterlab.\n",
    "\n",
    "\n",
    "We will also define a few other parameters including where to download the data to, and if we want the HDA-API functions to be verbose. **Lastly, we will tell the notebook where to find the query we will use to find the data.** These 'JSON' queries are what we use to ask WEkEO for data. They have a very specific form, but allow us quite fine grained control over what data to get. You can find the example one that we will use here: **JSON_templates/EO_EUM_DAT_SENTINEL-3_OL_2_WRR___.json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your WEkEO API username and password (needs to be in '  ')\n",
    "user_name = 'USERNAME'\n",
    "password = 'PASSWORD'\n",
    "\n",
    "# Generate an API key\n",
    "api_key = hapi.generate_api_key(user_name, password)\n",
    "display(HTML('Your API key is: <b>'+api_key+'</b>'))\n",
    "\n",
    "# set this key to true to download data.\n",
    "download_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the data should be downloaded to:\n",
    "download_dir_path = os.path.join(os.getcwd(),'products')\n",
    "# where we can find our data query form:\n",
    "JSON_query_dir = os.path.join(os.getcwd(),'JSON_templates')\n",
    "# HDA-API loud and noisy?\n",
    "verbose = False\n",
    "\n",
    "# make the output directory if required\n",
    "if not os.path.exists(download_dir_path):\n",
    "    os.makedirs(download_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have set how we want the script to run, we are ready to get some data. We start this process by telling the script what kind of data we want. In this case, this is OLCI L2 reduced resolution data, which has the following designation on WEkEO: **EO:EUM:DAT:SENTINEL-3:OL_2_WRR___**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLCI REDUCED RESOLUTION LEVEL 2 DATASET ID\n",
    "dataset_id = \"EO:EUM:DAT:SENTINEL-3:OL_2_WRR___\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use this dataset_id to find the correct, locally stored JSON query file which describes the data we want. The query file is called: **JSON_templates/cyano/EO_EUM_DAT_SENTINEL-3_OL_2_WRR___.json**\n",
    "\n",
    "You can edit this query if you want to get different data, but be aware of asking for too much data - you could be here a while and might run out of space to use this data in the JupyterLab. The box below gets the correct query file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find query file\n",
    "JSON_query_file = os.path.join(JSON_query_dir,dataset_id.replace(':','_')+\".json\")\n",
    "if not os.path.exists(JSON_query_file):\n",
    "    print('Query file ' + JSON_query_file + ' does not exist')\n",
    "else:\n",
    "    print('Found JSON query file for '+dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a query, we need to launch it to WEkEO to get our data. The box below takes care of this through the following steps:\n",
    "    1. initialise our HDA-API\n",
    "    2. get an access token for our data\n",
    "    3. accepts the WEkEO terms and conditions\n",
    "    4. loads our JSON query into memory\n",
    "    5. launches our search\n",
    "    6. waits for our search results\n",
    "    7. gets our result list\n",
    "    8. downloads our data\n",
    "\n",
    "This is quite a complex process, so much of the functionality has been buried 'behind the scenes'. If you want more information, you can check out the **wekeo-hda** tool kit in the parent training directory. The code below will report some information as it runs. At the end, it should tell you that one product has been downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_data:\n",
    "    HAPI_dict = hapi.init(dataset_id, api_key, download_dir_path)\n",
    "    HAPI_dict = hapi.get_access_token(HAPI_dict)\n",
    "    HAPI_dict = hapi.acceptTandC(HAPI_dict)\n",
    "\n",
    "    # load the query\n",
    "    with open(JSON_query_file, 'r') as f:\n",
    "        query = json.load(f)\n",
    "\n",
    "    # launch job\n",
    "    print('Launching job...')\n",
    "    HAPI_dict = hapi.get_job_id(HAPI_dict, query)\n",
    "\n",
    "    # check results\n",
    "    print('Getting results...')\n",
    "    HAPI_dict = hapi.get_results_list(HAPI_dict)\n",
    "    HAPI_dict = hapi.get_order_ids(HAPI_dict)\n",
    "\n",
    "    # download data\n",
    "    print('Downloading data...')\n",
    "    HAPI_dict = hapi.download_data(HAPI_dict, file_extension='.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentinel data is usually distributed as a zip file, which contains the SAFE format data within. To use this, we must unzip the file. The box below handles this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_data:\n",
    "    # unzip file\n",
    "    for filename in HAPI_dict['filenames']:\n",
    "        if os.path.splitext(filename)[-1] == '.zip':\n",
    "            print('Unzipping file')\n",
    "            try:\n",
    "                with ZipFile(filename, 'r') as zipObj:\n",
    "                    # Extract all the contents of zip file in current directory\n",
    "                    zipObj.extractall(os.path.dirname(filename))\n",
    "\n",
    "                # clear up the zip file\n",
    "                os.remove(filename)\n",
    "            except:\n",
    "                print(\"Failed to unzip....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_data:\n",
    "    unzipped_file = HAPI_dict['filenames'][0].replace('.zip','.SEN3')\n",
    "else:\n",
    "    unzipped_file = glob.glob(os.path.join(download_dir_path,'*OL_2_WRR*.SEN3'))\n",
    "\n",
    "input_root    = os.path.dirname(unzipped_file)\n",
    "input_path    = os.path.basename(unzipped_file)\n",
    "file_name_chl = 'chl_nn.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll quickly check, in the next box, if your data path is ok, and that the data file exists check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick path length check (some windows versions have a problem with long file paths)\n",
    "if len(os.path.join(input_root,input_path,file_name_chl)) > 259 \\\n",
    "  or len(os.path.join(input_root,input_path,file_name_chl)) > 248:\n",
    "    print('Beware, your path name is quite long. Consider moving your data to a new directory')\n",
    "else:\n",
    "    print('Path length name seems fine')\n",
    "    \n",
    "if os.path.exists(os.path.join(input_root,input_path,file_name_chl)):\n",
    "    print('Found the required data file')\n",
    "else:\n",
    "    print('Data file missing. Please check your path and file name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the file using functions from the netCDF4 (alias \"nc\") library. Note that to use a library in python you use the imported alias followed by a dot, and then the function you want (e.g. nc.Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLCI_file = xr.open_dataset(os.path.join(input_root,input_path,file_name_chl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access a variable you can use the following command, where the name of the variable you are interested in, follows the hash. If you remove the hash in the following box, put the cursor after the dot and hit 'tab' you will be presented with a list of all of the variables and methods associated with the OLCI_file object. Python is an 'object orientatated' language, which means that all objects have relevant methods associated with them. \n",
    "\n",
    "note: If you want to run all this code in one go, remember to put the hash back at the start of this line beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLCI_file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, lets load in some data, and then close our data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHL     = OLCI_file.CHL_NN.data\n",
    "OLCI_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look at the variables in your workspace in interactive python \n",
    "environments (like this, or ipython) by typing 'whos'. This will tell you the name of the variable, it's type and then information on it, such as its size and shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at our data.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(CHL);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not the prettiest plot - Python can do much better. For a start, we may wish to look at a smaller area. We'll do this now, using the relevant indexes for area of data you wish to use. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row1=3000\n",
    "row2=7000\n",
    "col1=0\n",
    "col2=3000\n",
    "grid_factor=2\n",
    "CHL_subset = CHL[row1:row2:grid_factor, col1:col2:grid_factor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(CHL_subset);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice a few problems with displaying plots like this. Firstly - they don't look very pretty (the colour scheme is not ideal, it is hard to see the coastline, and you can't differentiate the land from cloud), and secondly - the axes don't provide any information on the location (other than within the array) and there is no colour bar. \n",
    "\n",
    "To make a better plot we will need to add a few more tools to the libraries/modules we've currently imported. Below are a few lines of code to import cartopy (which will help us make a better, map based plot) and a few other tools to tweak how the data is displayed. \n",
    "\n",
    "The Cartopy is module allows us to use map projections to display data in a geographically relevant way. For those that are familiar to python, Cartopy has largely replaced the Basemap library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "land_resolution = '50m'\n",
    "land_poly = cfeature.NaturalEarthFeature('physical', 'land', land_resolution,\n",
    "                                        edgecolor='k',\n",
    "                                        facecolor=cfeature.COLORS['land'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need to load other data to make the plot - the longitude and latitude data associated with each pixel of the chlorophyll data. This data can be found in the geo_coordinates.nc file, within each S3 OLCI L2 folder. We load this in a very similar way to how we loaded the chlorophyll data, just with different file and variable names. The data path remains the same, referring the folder that contains all the netcdf files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_geo = 'geo_coordinates.nc'\n",
    "GEO_file      = xr.open_dataset(os.path.join(input_root,input_path,file_name_geo))\n",
    "LAT           = GEO_file.variables['latitude'][:]\n",
    "LON           = GEO_file.variables['longitude'][:]\n",
    "LAT_subset    = LAT[row1:row2:grid_factor, col1:col2:grid_factor]\n",
    "LON_subset    = LON[row1:row2:grid_factor, col1:col2:grid_factor]\n",
    "GEO_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to initialise the map we will use for plotting. The important things to choose here are:\n",
    "\n",
    "1. the projection you wish to use (this may depend on your region of interest, particularly if you are looking at polar data, more information about the different projects is available here: https://scitools.org.uk/cartopy/docs/v0.15/crs/projections.html)\n",
    "2. The limits of your map (by default, this will be set to your data limits)\n",
    "3. The resolution of the map coastline.\n",
    "\n",
    "See more information about the options for Cartopy here: https://scitools.org.uk/cartopy/docs/latest/\n",
    "\n",
    "(If you don't have cartopy installed, you can type \"conda install -c scitools/label/archive cartopy\" in your command prompt, with the anaconda distribution).\n",
    "\n",
    "We start by defining a figure (line 1 below) and then defining a map projection (line 2). All mapping instructions are taken care of using our map object 'm'. Now we make the plot (this may take some time to draw!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also save the figure using the code below (this will save in the folder where you are running the code, if you want to save elsewhere you need to specify the path).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure(figsize=(20, 20), dpi=300)\n",
    "m = plt.axes(projection=ccrs.PlateCarree(central_longitude=0.0))\n",
    "f1 = plt.pcolormesh(LON_subset,LAT_subset,np.ma.masked_invalid(CHL_subset), shading='flat', vmin=np.log10(0.01), vmax=np.log10(50), cmap=plt.cm.viridis)  \n",
    "m.coastlines(resolution=land_resolution, color='black', linewidth=1)\n",
    "m.add_feature(land_poly)\n",
    "g1 = m.gridlines(draw_labels = True)\n",
    "g1.xlabels_top = False\n",
    "g1.xlabel_style = {'size': 16, 'color': 'gray'}\n",
    "g1.ylabel_style = {'size': 16, 'color': 'gray'}\n",
    "cbar = plt.colorbar(f1, orientation=\"horizontal\", fraction=0.05, pad=0.07, ticks=[np.log10(0.01), np.log10(0.1),np.log10(0.5), np.log10(1),np.log10(3),np.log10(10),np.log10(50)]) \n",
    "cbar.ax.set_xticklabels(['0.01','0.1','0.5','1','3','10','50'], fontsize=20) \n",
    "cbar.set_label('Chlorophyll, mg.m$^{-3}$', fontsize=20)\n",
    "plt.title('OLCI [CHL_NN] mg.m$^{-3}$', fontsize=20);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2.savefig('OLCI_CHL_spatial_demo_no_flags.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this data is not flag masked. This means that we may have data that is subject to glint, or cloud, or a variety of other conditions that variously undermine quality. So, lets apply some flags. We are going to flag extensively, removing all data that corresponds to the following conditions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags_we_want = ['CLOUD', 'CLOUD_AMBIGUOUS', 'CLOUD_MARGIN', 'INVALID', 'COSMETIC', 'SATURATED', 'SUSPECT',\n",
    "                 'HISOLZEN', 'HIGHGLINT', 'SNOW_ICE', 'AC_FAIL', 'WHITECAPS', 'ANNOT_ABSO_D', 'ANNOT_MIXR1',\n",
    "                 'ANNOT_TAU06']\n",
    "\n",
    "file_name_flags = 'wqsf.nc'\n",
    "FLAG_file = xr.open_dataset(os.path.join(input_root,input_path,file_name_flags))\n",
    "# get all the flag names\n",
    "flag_names = FLAG_file['WQSF'].flag_meanings.split(' ')\n",
    "# get all the flag bit values\n",
    "flag_vals = FLAG_file['WQSF'].flag_masks\n",
    "# get the flag field itself\n",
    "FLAGS = FLAG_file.variables['WQSF'].data\n",
    "FLAG_file.close()\n",
    "\n",
    "# make the flag mask using the function we defined above \"flag_data_fast\"\n",
    "flag_mask = flag_data_fast(flags_we_want, flag_names, flag_vals, FLAGS, flag_type='WQSF')\n",
    "flag_mask = flag_mask.astype(float)\n",
    "flag_mask[flag_mask == 0.0] = np.nan\n",
    "\n",
    "# subset the flag mask\n",
    "FLAG_subset = flag_mask[row1:row2:grid_factor, col1:col2:grid_factor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we apply the flag data to our data and plot again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHL_subset[np.isfinite(FLAG_subset)] = np.nan\n",
    "\n",
    "fig3 = plt.figure(figsize=(20, 20), dpi=300)\n",
    "m = plt.axes(projection=ccrs.PlateCarree(central_longitude=0.0))\n",
    "f1 = plt.pcolormesh(LON_subset,LAT_subset,np.ma.masked_invalid(CHL_subset), shading='flat', vmin=np.log10(0.01), vmax=np.log10(50), cmap=plt.cm.viridis)  \n",
    "m.coastlines(resolution=land_resolution, color='black', linewidth=1)\n",
    "m.add_feature(land_poly)\n",
    "g1 = m.gridlines(draw_labels = True)\n",
    "g1.xlabels_top = False\n",
    "g1.xlabel_style = {'size': 16, 'color': 'gray'}\n",
    "g1.ylabel_style = {'size': 16, 'color': 'gray'}\n",
    "cbar = plt.colorbar(f1, orientation=\"horizontal\", fraction=0.05, pad=0.07, ticks=[np.log10(0.01), np.log10(0.1),np.log10(0.5), np.log10(1),np.log10(3),np.log10(10),np.log10(50)]) \n",
    "cbar.ax.set_xticklabels(['0.01','0.1','0.5','1','3','10','50'], fontsize=20) \n",
    "cbar.set_label('Chlorophyll, mg.m$^{-3}$', fontsize=20)\n",
    "plt.title('OLCI [CHL_NN] mg.m$^{-3}$', fontsize=20);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3.savefig('OLCI_CHL_spatial_demo_flags.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flags_we_want variable can be customised to any flag combination required, and, if you wish, the box above can be adapted to plot flags instead of the CHL field. You could also run the flags_we_want routine with each flag individually, to get a mask for every flag. That can then be used in more advanced plotting. But thats up to you..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/all_partners_wekeo.png' alt='' align='center' width='75%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:left;\">This project is licensed under the <a href=\"./LICENSE\">MIT License</a> <span style=\"float:right;\"><a href=\"https://github.com/wekeo/wekeo-jupyter-lab\">View on GitHub</a> | <a href=\"https://www.wekeo.eu/\">WEkEO Website</a> | <a href=mailto:support@wekeo.eu>Contact</a></span></p>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
